{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9Fdx2lrsrvtiHrl6xbqje",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1997MarsRover/cuda-guide/blob/main/llm_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzNdWzkzcqyw",
        "outputId": "1233c980-7dff-4ad8-e4a8-92d19643cfbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm.c'...\n",
            "remote: Enumerating objects: 280, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 280 (delta 97), reused 78 (delta 57), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (280/280), 130.01 KiB | 13.00 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "/content/llm.c\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/llm.c.git\n",
        "%cd llm.c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj0yDcD2dQ-1",
        "outputId": "96bf48db-2343-4b6e-d5f4-634abedce373"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepro_tinyshakespeare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QibxpClrdvyn",
        "outputId": "10cdbaa7-bafd-4cad-a45f-041dc6d5c9b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt to data/tiny_shakespeare.txt...\n",
            "data/tiny_shakespeare.txt: 1.06MiB [00:00, 29.3MiB/s]     \n",
            "Saved 32768 tokens to data/tiny_shakespeare_val.bin\n",
            "Saved 305260 tokens to data/tiny_shakespeare_train.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_gpt2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSRsK-dzd0k8",
        "outputId": "a40566c9-a1f8-46fd-8b9f-3a336f049874"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loading weights from pretrained gpt: gpt2\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.02MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:01<00:00, 329MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 608kB/s]\n",
            "loading cached tokens in data/tiny_shakespeare_val.bin\n",
            "wrote gpt2_124M.bin\n",
            "wrote gpt2_124M_debug_state.bin\n",
            "iteration 0, loss: 5.270007133483887, time: 5164.209ms\n",
            "iteration 1, loss: 4.05969762802124, time: 134.554ms\n",
            "iteration 2, loss: 3.375108003616333, time: 96.441ms\n",
            "iteration 3, loss: 2.8007514476776123, time: 95.854ms\n",
            "iteration 4, loss: 2.3153576850891113, time: 92.914ms\n",
            "iteration 5, loss: 1.849000334739685, time: 92.409ms\n",
            "iteration 6, loss: 1.3946236371994019, time: 91.147ms\n",
            "iteration 7, loss: 0.9991112947463989, time: 92.524ms\n",
            "iteration 8, loss: 0.6240730881690979, time: 91.686ms\n",
            "iteration 9, loss: 0.3765047788619995, time: 92.493ms\n",
            "<|endoftext|>One year ago today:\n",
            "This is the first week since we last spoke.\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make train_gpt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUVPVNwueaTO",
        "outputId": "9d7e6268-4b9e-45d8-92d7-442d17f6f3fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NICE Compiling with OpenMP support\n",
            "cc -O3 -Ofast -Wno-unused-result -fopenmp -DOMP   train_gpt2.c -lm -lgomp -o train_gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!OMP_NUM_THREADS=8 ./train_gpt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xlMTIePesXf",
        "outputId": "19c5054f-5412-46ae-f0d9-aeec90ad2460"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPT-2]\n",
            "max_seq_len: 1024\n",
            "vocab_size: 50257\n",
            "num_layers: 12\n",
            "num_heads: 12\n",
            "channels: 768\n",
            "num_parameters: 124439808\n",
            "train dataset num_batches: 1192\n",
            "val dataset num_batches: 128\n",
            "num_activations: 73323776\n",
            "val loss 5.252006\n",
            "step 0: train loss 5.356172 (took 26772.736942 ms)\n",
            "Error: must forward with targets before backward\n"
          ]
        }
      ]
    }
  ]
}